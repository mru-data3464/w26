{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc86a755",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "## Anscombe's Quartet\n",
    "The following code is adapted from https://matplotlib.org/stable/gallery/specialty_plots/anscombe.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cdf720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a041e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5])\n",
    "y1 = np.array([8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68])\n",
    "y2 = np.array([9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74])\n",
    "y3 = np.array([7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73])\n",
    "x4 = np.array([8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8])\n",
    "y4 = np.array([6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89])\n",
    "\n",
    "datasets = {\n",
    "    'I': (x, y1),\n",
    "    'II': (x, y2),\n",
    "    'III': (x, y3),\n",
    "    'IV': (x4, y4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at mean, stdev, corr coeff\n",
    "for label, (x,y) in datasets.items():\n",
    "    print(f\"Dataset {label}:\")\n",
    "    print(f\"  Mean:  {np.mean(y):.2f}\")\n",
    "    print(f\"  Stdev: {np.std(y):.2f}\")\n",
    "    print(f\"  R^2: {np.corrcoef(x,y)[0][1]**2:.2f}\")\n",
    "    # print(f\"  Min: {np.min(y):.2f}\")\n",
    "    # print(f\"  Max: {np.max(y):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a964a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(6, 6),\n",
    "                        gridspec_kw={'wspace': 0.08, 'hspace': 0.08})\n",
    "axs[0, 0].set(xlim=(0, 20), ylim=(2, 14))\n",
    "axs[0, 0].set(xticks=(0, 10, 20), yticks=(4, 8, 12))\n",
    "\n",
    "for ax, (label, (x, y)) in zip(axs.flat, datasets.items()):\n",
    "    ax.text(0.1, 0.9, label, fontsize=20, transform=ax.transAxes, va='top')\n",
    "    ax.tick_params(direction='in', top=True, right=True)\n",
    "    ax.plot(x, y, 'o')\n",
    "\n",
    "    # linear regression\n",
    "    p1, p0 = np.polyfit(x, y, deg=1)  # slope, intercept\n",
    "    ax.axline(xy1=(0, p0), slope=p1, color='r', lw=2)\n",
    "\n",
    "    # add text box for the statistics\n",
    "    stats = (f'$\\\\mu$ = {np.mean(y):.2f}\\n'\n",
    "             f'$\\\\sigma$ = {np.std(y):.2f}\\n'\n",
    "             f'$r$ = {np.corrcoef(x, y)[0][1]:.2f}')\n",
    "    bbox = dict(boxstyle='round', fc='blanchedalmond', ec='orange', alpha=0.5)\n",
    "    ax.text(0.95, 0.07, stats, fontsize=9, bbox=bbox,\n",
    "            transform=ax.transAxes, horizontalalignment='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953bce52",
   "metadata": {},
   "source": [
    "## Datasaurus Dozen\n",
    "From https://www.research.autodesk.com/publications/same-stats-different-graphs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1af383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dsd = pd.read_csv(\"DatasaurusDozen.tsv\", sep=\"\\t\")\n",
    "dsd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsd[\"dataset\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e365d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 4, sharex=True, sharey=True, figsize=(10,8))\n",
    "\n",
    "for ax, name in zip(axs.flat, dsd[\"dataset\"].unique()):\n",
    "    slice = dsd.query(\"dataset == @name\")\n",
    "    ax.scatter(slice[\"x\"], slice[\"y\"])\n",
    "    ax.set_title(name)\n",
    "\n",
    "    print(f\"Dataset {name}:\")\n",
    "    print(f\"  Mean:  {np.mean(slice['y']):.2f}\")\n",
    "    print(f\"  Stdev: {np.std(slice['y']):.2f}\")\n",
    "    print(f\"  R^2: {np.corrcoef(slice['x'],slice['y'])[0][1]**2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27269b",
   "metadata": {},
   "source": [
    "# A more useful example\n",
    "Let's use Kaggle's [Titanic dataset](https://www.kaggle.com/competitions/titanic/data). Kaggle has already split the data into train/test, so we'll just load the training data and pretend that test doesn't exist.\n",
    "\n",
    "Many public datasets are pre-split. This is so that you can replicate published results exactly. In other cases (like Kaggle competitions), a final test set is kept secret, ensuring things really can't leak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3269095",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = pd.read_csv(\"titanic_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fd4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info, describe, head\n",
    "td.info()\n",
    "td.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1476a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do more women survive than men? \"Women and children first\"\n",
    "td[[\"Sex\", \"Survived\"]].groupby(\"Sex\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e727b",
   "metadata": {},
   "source": [
    "Yes, looks like sex is related to survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about the relationship between age and survival?\n",
    "ax = td[td[\"Survived\"] == 0].hist(by=\"Sex\", column=\"Age\", figsize=(8,4), alpha=0.3, label=\"Did not survive\")\n",
    "td[td[\"Survived\"] == 1].hist(ax=ax, by=\"Sex\", column=\"Age\", figsize=(8,4), alpha=0.3, label=\"Survived\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"# of people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc893bd",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573412be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data - random sample\n",
    "td_train, td_test = train_test_split(td, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc1024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(td_train[\"Survived\"].value_counts() / len(td_train))\n",
    "print(td_test[\"Survived\"].value_counts() / len(td_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e36546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data - stratified sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121872fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data - hashing to prevent changes\n",
    "from zlib import crc32\n",
    "\n",
    "# Passenger ID should be a unique identifier, but let's double check\n",
    "print(\"Unique identifier:\", td[\"PassengerId\"].is_unique)\n",
    "\n",
    "h_train_ids = td[\"PassengerId\"].apply(lambda n: crc32(str(n).encode())) < 0.8 * 2**32\n",
    "h_test_ids = np.ones_like(td[\"PassengerId\"].values, dtype=bool)\n",
    "h_test_ids[h_train_ids] = 0\n",
    "\n",
    "print(f\"Training fraction: {h_train_ids.sum() / len(td)}\")\n",
    "print(f\"Testing fraction: {h_test_ids.sum() / len(td)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75940ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plots\n",
    "td.plot.scatter(x=\"Age\", y=\"Fare\", c=\"Survived\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe a log scale would help?\n",
    "ax = td.plot.scatter(x=\"Age\", y=\"Fare\", c=\"Survived\", cmap=\"viridis\")\n",
    "ax.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a1ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots\n",
    "td.plot.box(by=\"Survived\", layout=(2,3), figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a8db9",
   "metadata": {},
   "source": [
    "## Extra plots\n",
    "Small examples that don't really fit with the main flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644468c4",
   "metadata": {},
   "source": [
    "A brief example of sampling bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p = 0.8 # ratio of likes cilantro to dislikes cilantro\n",
    "buffer = 0.05 # plus/minus 5%\n",
    "sample_sizes = [10, 100, 200, 500, 1000, 10000]\n",
    "prob_bias = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    too_small = n * (p - buffer)\n",
    "    too_large = n * (p + buffer)\n",
    "    proba_too_small = binom(n, p).cdf(too_small - 1)\n",
    "    proba_too_large = 1 - binom(n, p).cdf(too_large)\n",
    "    prob_bias.append((proba_too_small + proba_too_large) * 100)\n",
    "\n",
    "plt.plot(sample_sizes, prob_bias, \"o-\")\n",
    "plt.xlabel(\"Sample size\")\n",
    "plt.ylabel(\"Probability of random sampling bias (%)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "for n, b in zip(sample_sizes, prob_bias):\n",
    "    print(f\"Sample size {n}: Bias = {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "ids = np.arange(10)\n",
    "train = rng.choice(ids, 8, replace=False)\n",
    "test = np.delete(ids, train)\n",
    "\n",
    "print(\"Train:\", train)\n",
    "print(\"Test: \", test)\n",
    "\n",
    "plt.bar(train, np.ones_like(train), label=\"train\")\n",
    "plt.bar(test, np.ones_like(test), label=\"test\")\n",
    "plt.ylim([0,1.2])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e6fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deterministic approach: hashing\n",
    "from zlib import crc32\n",
    "\n",
    "# 80% of the maximum possible 32-bit hash value\n",
    "test_thresh = 0.8 * 2**32\n",
    "\n",
    "hash_vals = np.array([crc32(id) for id in ids])\n",
    "train = ids[hash_vals < test_thresh]\n",
    "test = np.delete(ids, train)\n",
    "\n",
    "print(\"Train:\", train)\n",
    "print(\"Test: \", test)\n",
    "\n",
    "plt.bar(train, np.ones_like(train), label=\"train\")\n",
    "plt.bar(test, np.ones_like(test), label=\"test\")\n",
    "plt.ylim([0,1.2])\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
